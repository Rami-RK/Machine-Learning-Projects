{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn. __version__  # Note: SMOTE library requires sklearn version ='0.23.1' or higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information about Data Set \n",
    "* Source : https://www.kaggle.com/mlg-ulb/creditcardfraud?select=creditcard.csv\n",
    "\n",
    "* It is a CSV file, contains 31 features, the last feature is used to classify the transaction whether it is a fraud or not. The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "* It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues original features are not provided and more background information about the data is also not present. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1727485630620034"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Percentage of Fraud Data i.e. data with Class 1\n",
    "sum(df.Class==1)*100/len(df.Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this result we can see that data have a very imbalanced class - just 0.17% of our dataset belong to the target class!\n",
    "\n",
    "This is a problem because many machine learning models are designed to maximize overall accuracy, which especially with imbalanced classes may not be the best metric to use. Classification accuracy is defined as the number of correct predictions divided by total predictions times 100. For example, if we simply predicted all transactions are not fraud, we would get a classification acuracy score of over 99%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling Time and amount data\n",
    "* Since most of our data has already been scaled we should scale the columns that are left to scale i.e 'Amount' and 'Time' columns.\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# RobustScaler is less prone to outliers.\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "scaled_amount= rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "scaled_time= rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "\n",
    "df.insert(0, 'scaled_amount', scaled_amount)\n",
    "df.insert(1, 'scaled_time', scaled_time)\n",
    "\n",
    "df.drop(['Time','Amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.783274</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.269825</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.983721</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.418291</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.670579</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "0       1.783274    -0.994983 -1.359807 -0.072781  2.536347  1.378155   \n",
       "1      -0.269825    -0.994983  1.191857  0.266151  0.166480  0.448154   \n",
       "2       4.983721    -0.994972 -1.358354 -1.340163  1.773209  0.379780   \n",
       "3       1.418291    -0.994972 -0.966272 -0.185226  1.792993 -0.863291   \n",
       "4       0.670579    -0.994960 -1.158233  0.877737  1.548718  0.403034   \n",
       "\n",
       "         V5        V6        V7        V8  ...       V20       V21       V22  \\\n",
       "0 -0.338321  0.462388  0.239599  0.098698  ...  0.251412 -0.018307  0.277838   \n",
       "1  0.060018 -0.082361 -0.078803  0.085102  ... -0.069083 -0.225775 -0.638672   \n",
       "2 -0.503198  1.800499  0.791461  0.247676  ...  0.524980  0.247998  0.771679   \n",
       "3 -0.010309  1.247203  0.237609  0.377436  ... -0.208038 -0.108300  0.005274   \n",
       "4 -0.407193  0.095921  0.592941 -0.270533  ...  0.408542 -0.009431  0.798278   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  Class  \n",
       "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Splitting Data into Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Class\n",
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique predicted labels:  [0]\n",
      "Test score:  0.9981461194910255\n"
     ]
    }
   ],
   "source": [
    "# DummyClassifier to predict only target 0\n",
    "dummy = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "dummy_pred = dummy.predict(X_test)\n",
    "\n",
    "# checking unique labels\n",
    "print('Unique predicted labels: ', (np.unique(dummy_pred)))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test score: ', accuracy_score(y_test, dummy_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted our accuracy score for classifying all transactions as not fraud is 99.8%!\n",
    "\n",
    "As the Dummy Classifier predicts only Class 0, it is clearly not a good option for our objective of correctly classifying fraudulent transactions.\n",
    "\n",
    "### 1. Let's see how Logistic Regression performs on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling the data as is\n",
    "# Train model\n",
    "lr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    " \n",
    "# Predict on training set\n",
    "lr_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992275497879273"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=int64), array([71111,    91], dtype=int64))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking unique values\n",
    "np.unique(lr_pred,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    71111\n",
       "1       91\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking unique values :Another Way\n",
    "predictions = pd.DataFrame(lr_pred)\n",
    "predictions[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71197</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71198</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71199</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71200</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71201</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71202 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "...   ..\n",
       "71197  0\n",
       "71198  0\n",
       "71199  0\n",
       "71200  0\n",
       "71201  0\n",
       "\n",
       "[71202 rows x 1 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression outperformed the Dummy Classifier! We can see that it predicted 94 instances of class 1, so this is definitely an improvement. But can we do better?\n",
    "\n",
    "Let's see if we can apply some techniques for dealing with class imbalance to improve these results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the performance metric\n",
    "Accuracy is not the best metric to use when evaluating imbalanced datasets as it can be misleading. Metrics that can provide better insight include:\n",
    "\n",
    "* Confusion Matrix: a table showing correct predictions and types of incorrect predictions.\n",
    "* Precision: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier's exactness. Low precision indicates a high number of false positives.\n",
    "* Recall: the number of true positives divided by the number of positive values in the test data. Recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier's completeness. Low recall indicates a high number of false negatives.\n",
    "* F1: Score: the weighted average of precision and recall.\n",
    "Since our main objective with the dataset is to prioritize accuraltely classifying fraud cases the recall score can be considered our main metric to use for evaluating outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.7533632286995515 \n",
      "\n",
      "confusion matrix: \n",
      "        0   1\n",
      "0  71063   7\n",
      "1     48  84 \n",
      "\n",
      "recall score:  0.6363636363636364\n"
     ]
    }
   ],
   "source": [
    "# f1 score\n",
    "print('f1 score: ',f1_score(y_test, lr_pred),'\\n')\n",
    "# confusion matrix\n",
    "print('confusion matrix: \\n',pd.DataFrame(confusion_matrix(y_test, lr_pred)),'\\n')\n",
    "# recall score\n",
    "print('recall score: ',recall_score(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a very high accuracy score of 0.999 but a F1 score of only 0.752. And from the confusion matrix, we can see we are misclassifying several observations leading to a recall score of only 0.64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Precision Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c1c9208>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "pred_prob=lr.predict_proba(X_test)\n",
    "y_score = pred_prob[:,1]\n",
    "precision,recall, thresholds = precision_recall_curve(y_test, y_score)\n",
    "plt.plot(recall,precision, color='red', alpha=0.5,linewidth=1.5,label='Logistic Regression')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.legend(loc='lower left', fontsize = 'large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'True Positive Rate')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "pred_prob=lr.predict_proba(X_test)\n",
    "y_score = pred_prob[:,1]\n",
    "fpr,tpr,thresholds =roc_curve(y_test, y_score)\n",
    "plt.plot(fpr,tpr,linewidth=2)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under the Curve (AUC)\n",
    "* One way to compare the classifiers is to measure the Area Under the Curve (AUC).A perfect classifier will have a ROC AUC equal to 1 wheres a purely random classifier will have a ROC AUC =1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9587288034417625"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Change the Algorithm: Trying  RandomForestClassifier\n",
    "While in every machine learning problem, its a good rule of thumb to try a variety of algorithms, it can be especially beneficial with imbalanced datasets. Decision trees frequently perform well on imbalanced data. They work by learning a hierachy of if/else questions. This can force both classes to be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999592708069998"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, rfc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.879668049792531 \n",
      "\n",
      "confusion matrix: \n",
      "        0    1\n",
      "0  71067    3\n",
      "1     26  106 \n",
      "\n",
      "recall score:  0.803030303030303\n"
     ]
    }
   ],
   "source": [
    "# f1 score\n",
    "print('f1 score: ',f1_score(y_test, rfc_pred),'\\n')\n",
    "# confusion matrix\n",
    "print('confusion matrix: \\n',pd.DataFrame(confusion_matrix(y_test, rfc_pred)),'\\n')\n",
    "# recall score\n",
    "print('recall score: ',recall_score(y_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Resampling Techniques\n",
    "### 3.1. Oversampling Minority Class\n",
    "Oversampling can be defined as adding more copies of the minority class. Oversampling can be a good choice when you don't have a ton of data to work with. A con to consider when undersampling is that it can cause overfitting and poor generalization to your test set.\n",
    "\n",
    "We will use the resampling module from Scikit-Learn to randomly replicate samples from the minority class.\n",
    "#### Important Note\n",
    "Always split into test and train sets BEFORE trying any resampling techniques! Oversampling before splitting the data can allow the exact same observations to be present in both the test and train sets! This can allow our model to simply memorize specific data points and cause overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input features and target\n",
    "y = df.Class\n",
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264873</th>\n",
       "      <td>161634.0</td>\n",
       "      <td>-0.395578</td>\n",
       "      <td>1.489129</td>\n",
       "      <td>-0.833442</td>\n",
       "      <td>-0.224271</td>\n",
       "      <td>0.369444</td>\n",
       "      <td>-1.453886</td>\n",
       "      <td>0.796593</td>\n",
       "      <td>-0.060403</td>\n",
       "      <td>0.338270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231624</td>\n",
       "      <td>0.955194</td>\n",
       "      <td>-0.172092</td>\n",
       "      <td>-0.041050</td>\n",
       "      <td>-0.313444</td>\n",
       "      <td>-0.174301</td>\n",
       "      <td>0.064657</td>\n",
       "      <td>-0.036960</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163821</th>\n",
       "      <td>116237.0</td>\n",
       "      <td>1.950487</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>-1.761814</td>\n",
       "      <td>1.232470</td>\n",
       "      <td>0.523175</td>\n",
       "      <td>-0.650657</td>\n",
       "      <td>0.504231</td>\n",
       "      <td>-0.200857</td>\n",
       "      <td>0.116805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086306</td>\n",
       "      <td>0.326297</td>\n",
       "      <td>-0.068839</td>\n",
       "      <td>-0.416589</td>\n",
       "      <td>0.426044</td>\n",
       "      <td>-0.486299</td>\n",
       "      <td>-0.031266</td>\n",
       "      <td>-0.072543</td>\n",
       "      <td>38.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72083</th>\n",
       "      <td>54557.0</td>\n",
       "      <td>1.105167</td>\n",
       "      <td>-0.166253</td>\n",
       "      <td>0.569520</td>\n",
       "      <td>0.681043</td>\n",
       "      <td>-0.259189</td>\n",
       "      <td>0.642792</td>\n",
       "      <td>-0.437034</td>\n",
       "      <td>0.356746</td>\n",
       "      <td>0.441417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009073</td>\n",
       "      <td>0.293023</td>\n",
       "      <td>-0.028688</td>\n",
       "      <td>-0.242206</td>\n",
       "      <td>0.389813</td>\n",
       "      <td>0.482852</td>\n",
       "      <td>0.010705</td>\n",
       "      <td>-0.008399</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196949</th>\n",
       "      <td>131771.0</td>\n",
       "      <td>1.805238</td>\n",
       "      <td>0.961264</td>\n",
       "      <td>-1.717212</td>\n",
       "      <td>4.094625</td>\n",
       "      <td>0.938666</td>\n",
       "      <td>-0.227785</td>\n",
       "      <td>0.152911</td>\n",
       "      <td>0.066753</td>\n",
       "      <td>-1.073784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137875</td>\n",
       "      <td>-0.450959</td>\n",
       "      <td>0.098530</td>\n",
       "      <td>-0.662272</td>\n",
       "      <td>-0.150154</td>\n",
       "      <td>-0.098852</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.017622</td>\n",
       "      <td>37.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126213</th>\n",
       "      <td>77959.0</td>\n",
       "      <td>0.835421</td>\n",
       "      <td>-1.191847</td>\n",
       "      <td>0.578455</td>\n",
       "      <td>0.586101</td>\n",
       "      <td>-1.236663</td>\n",
       "      <td>0.194617</td>\n",
       "      <td>-0.532404</td>\n",
       "      <td>0.061561</td>\n",
       "      <td>-0.734344</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072349</td>\n",
       "      <td>-0.109154</td>\n",
       "      <td>-0.308356</td>\n",
       "      <td>0.011968</td>\n",
       "      <td>0.461350</td>\n",
       "      <td>-0.244810</td>\n",
       "      <td>0.031845</td>\n",
       "      <td>0.060910</td>\n",
       "      <td>237.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "264873  161634.0 -0.395578  1.489129 -0.833442 -0.224271  0.369444 -1.453886   \n",
       "163821  116237.0  1.950487  0.002312 -1.761814  1.232470  0.523175 -0.650657   \n",
       "72083    54557.0  1.105167 -0.166253  0.569520  0.681043 -0.259189  0.642792   \n",
       "196949  131771.0  1.805238  0.961264 -1.717212  4.094625  0.938666 -0.227785   \n",
       "126213   77959.0  0.835421 -1.191847  0.578455  0.586101 -1.236663  0.194617   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "264873  0.796593 -0.060403  0.338270  ...  0.231624  0.955194 -0.172092   \n",
       "163821  0.504231 -0.200857  0.116805  ...  0.086306  0.326297 -0.068839   \n",
       "72083  -0.437034  0.356746  0.441417  ...  0.009073  0.293023 -0.028688   \n",
       "196949  0.152911  0.066753 -1.073784  ... -0.137875 -0.450959  0.098530   \n",
       "126213 -0.532404  0.061561 -0.734344  ... -0.072349 -0.109154 -0.308356   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "264873 -0.041050 -0.313444 -0.174301  0.064657 -0.036960    2.74      0  \n",
       "163821 -0.416589  0.426044 -0.486299 -0.031266 -0.072543   38.44      0  \n",
       "72083  -0.242206  0.389813  0.482852  0.010705 -0.008399    1.00      0  \n",
       "196949 -0.662272 -0.150154 -0.098852 -0.000030  0.017622   37.89      0  \n",
       "126213  0.011968  0.461350 -0.244810  0.031845  0.060910  237.00      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate our training data back together\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    213245\n",
       "0    213245\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate minority and majority classes\n",
    "not_fraud = X[X.Class==0]\n",
    "fraud = X[X.Class==1]\n",
    "\n",
    "# upsample minority\n",
    "fraud_upsampled = resample(fraud,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(not_fraud), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([not_fraud, fraud_upsampled])\n",
    "\n",
    "# check new class counts\n",
    "upsampled.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9807589674447347"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying logistic regression again with the balanced dataset\n",
    "y_train = upsampled.Class\n",
    "X_train = upsampled.drop('Class', axis=1)\n",
    "\n",
    "upsampled = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "\n",
    "upsampled_pred = upsampled.predict(X_test)\n",
    "accuracy_score(y_test, upsampled_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.14375000000000002 \n",
      "\n",
      "confusion matrix: \n",
      "        0     1\n",
      "0  69717  1353\n",
      "1     17   115 \n",
      "\n",
      "recall score:  0.8712121212121212\n"
     ]
    }
   ],
   "source": [
    "# f1 score\n",
    "print('f1 score: ',f1_score(y_test, upsampled_pred),'\\n')\n",
    "# confusion matrix\n",
    "print('confusion matrix: \\n',pd.DataFrame(confusion_matrix(y_test, upsampled_pred)),'\\n')\n",
    "# recall score\n",
    "print('recall score: ',recall_score(y_test, upsampled_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy score decreased after upsampling, but the model is now predicting both classes more equally, making it an improvement over our plain logistic regression above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Undersampling Majority Class\n",
    "Undersampling can be defined as removing some observations of the majority class. Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback to undersampling is that we are removing information that may be valuable.\n",
    "\n",
    "We will again use the resampling module from Scikit-Learn to randomly remove samples from the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    360\n",
       "0    360\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# still using our separated classes fraud and not_fraud from above\n",
    "\n",
    "# downsample majority\n",
    "not_fraud_downsampled = resample(not_fraud,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(fraud), # match minority n\n",
    "                                random_state = 27) # reproducible results\n",
    "\n",
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([not_fraud_downsampled, fraud])\n",
    "\n",
    "# checking counts\n",
    "downsampled.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9758574197354007"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying logistic regression again with the undersampled dataset\n",
    "\n",
    "y_train = downsampled.Class\n",
    "X_train = downsampled.drop('Class', axis=1)\n",
    "\n",
    "undersampled = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "\n",
    "undersampled_pred = undersampled.predict(X_test)\n",
    "accuracy_score(y_test,undersampled_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.11710323574730355 \n",
      "\n",
      "confusion matrix: \n",
      "        0     1\n",
      "0  69369  1701\n",
      "1     18   114 \n",
      "\n",
      "recall score:  0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "# f1 score\n",
    "print('f1 score: ',f1_score(y_test, undersampled_pred),'\\n')\n",
    "# confusion matrix\n",
    "print('confusion matrix: \\n',pd.DataFrame(confusion_matrix(y_test, undersampled_pred)),'\\n')\n",
    "# recall score\n",
    "print('recall score: ',recall_score(y_test, undersampled_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsampling produced a higher recall score than upsampling! One concern here is the small number of total samples we used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate Synthetic Samples: Using SMOTE\n",
    "SMOTE or Synthetic Minority Oversampling Technique is a popular algorithm to creates sythetic observations of the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install imbalanced-learn ## Install if not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input features and target\n",
    "y = df.Class\n",
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n",
    "\n",
    "X_train, y_train = SMOTE(random_state=27).fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9858571388444145"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smote = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "\n",
    "smote_pred = smote.predict(X_test)\n",
    "\n",
    "# Checking accuracy\n",
    "accuracy_score(y_test, smote_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.18461538461538463 \n",
      "\n",
      "confusion matrix: \n",
      "        0    1\n",
      "0  70081  989\n",
      "1     18  114\n",
      "recall score:  0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "# f1 score\n",
    "print('f1 score: ',f1_score(y_test, smote_pred),'\\n')\n",
    "# confusion matrix\n",
    "print('confusion matrix: \\n',pd.DataFrame(confusion_matrix(y_test, smote_pred)))\n",
    "# recall score\n",
    "print('recall score: ',recall_score(y_test, smote_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "We covered 4 different methods for dealing with imbalanced datasets: With the performance metric associated with Confusion Matrix\n",
    "\n",
    "* Oversampling minority class\n",
    "* Undersampling majority class\n",
    "* Change the algorithm\n",
    "* Generate synthetic samples\n",
    "\n",
    "###### These are just some of the many possible methods to try when dealing with imbalanced datasets, and not an exhaustive list. Some others methods to consider are collecting more data or choosing different resampling ratios - you don't have to have exactly a 1:1 ratio! You should always try several approaches and then decide which is best for your problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
